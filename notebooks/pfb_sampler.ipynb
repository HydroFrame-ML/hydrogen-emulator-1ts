{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "# Need to use xbatcher from: https://github.com/arbennett/xbatcher/tree/develop\n",
    "import xbatcher as xb\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from parflow.tools.io import read_pfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling all of the file paths we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/hydrodata/PFCLM/CONUS1_baseline/simulations'\n",
    "YEAR = 2004\n",
    "\n",
    "# Get Pressure files\n",
    "pressure_files = sorted(glob(f'{BASE_DIR}/{YEAR}/raw_outputs/pressure/*.pfb'))\n",
    "pressure_files = {\n",
    "    't': pressure_files[0:-1],\n",
    "    't+1': pressure_files[1:]\n",
    "}\n",
    "\n",
    "# Get parameter filesk\n",
    "parameter_names = [\n",
    "    'permeability', 'porosity', 'vgn_alpha', 'vgn_n', 'slope_x', 'slope_y'\n",
    "]\n",
    "parameter_files = {\n",
    "    name: f'{BASE_DIR}/static/CONUS1_{name}.pfb' for name in parameter_names\n",
    "}\n",
    "\n",
    "\n",
    "# Get forcing files\n",
    "all_forcings = glob(f'{BASE_DIR}/{YEAR}/WY{YEAR}/*.pfb')\n",
    "varnames = set([f.split('/')[-1].split('.')[1] for f in all_forcings])\n",
    "\n",
    "variable_forcings = {}\n",
    "for v in varnames:\n",
    "    variable_forcings[v] = sorted(glob(f'{BASE_DIR}/{YEAR}/WY{YEAR}/*.{v}.*pfb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some constants, these might need to be pulled from data directly or user configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_EXTENT = 3342\n",
    "Y_EXTENT = 1888\n",
    "T_EXTENT = 8759 # 1 less because we are predicting t+1\n",
    "Z_EXTENT = 5\n",
    "PATCH_SIZE = 128\n",
    "PATCH_OVERLAP = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy data will be used to pull indices for reading subsets of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = xr.Dataset().assign_coords({\n",
    "    'time': np.arange(T_EXTENT),\n",
    "    'z': np.arange(Z_EXTENT),\n",
    "    'y': np.arange(Y_EXTENT),\n",
    "    'x': np.arange(X_EXTENT)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The batch generator create all of the sets of indices we need to sample the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen = xb.BatchGenerator(\n",
    "    dummy_data,\n",
    "    input_dims={'x': PATCH_SIZE, 'y': PATCH_SIZE, 'time': 1},\n",
    "    input_overlap={'x': PATCH_OVERLAP, 'y': PATCH_OVERLAP},\n",
    "    return_partial=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Now you can see this pulls samples from the dummy data\n",
    "# Normally you would loop over this, but I can just do \n",
    "# this to grab the first sample\n",
    "sample_indices = next(iter(bgen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pulling the source and target pressure fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 128, 128) (5, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Pulling the indices we need\n",
    "time_index = sample_indices['time'].values[0]\n",
    "x_min, x_max = sample_indices['x'].values[[0, -1]]\n",
    "y_min, y_max = sample_indices['y'].values[[0, -1]]\n",
    "\n",
    "# Setting up the keys dictionary\n",
    "pressure_keys = {\n",
    "    'x': {'start': x_min, 'stop': x_max+1},\n",
    "    'y': {'start': y_min, 'stop': y_max+1},\n",
    "}\n",
    "\n",
    "# Construct the state data:\n",
    "file_to_read = pressure_files['t'][time_index]\n",
    "state_data = read_pfb(file_to_read, keys=pressure_keys)\n",
    "\n",
    "# Construct the target data:\n",
    "file_to_read_target = pressure_files['t+1'][time_index]\n",
    "target_data = read_pfb(file_to_read_target, keys=pressure_keys)\n",
    "\n",
    "# Forcings and targets now have dims\n",
    "# (layers, y, x)\n",
    "print(state_data.shape, target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling the forcing data and stacking it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 128, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate which forcing file to choose\n",
    "forcing_file_starts = np.arange(1, T_EXTENT, 24)\n",
    "next_forcing_file_index = np.where((forcing_file_starts - time_index) < 0)[0][-1]\n",
    "\n",
    "# Calculate what timestep to select out of the forcing file\n",
    "forcing_file_z_index = time_index - forcing_file_starts[next_forcing_file_index]\n",
    "\n",
    "# Put it in the keys format that read_pfb uses\n",
    "forcing_keys = {\n",
    "    'x': {'start': x_min, 'stop': x_max+1},\n",
    "    'y': {'start': y_min, 'stop': y_max+1},\n",
    "    # z acts as timesteps for forcing variables\n",
    "    'z': {'start': forcing_file_z_index, 'stop': forcing_file_z_index + 1},\n",
    "}\n",
    "\n",
    "# Construct the forcing data:\n",
    "forcing_data = []\n",
    "for var in variable_forcings.keys():\n",
    "    file_to_read = variable_forcings[var][next_forcing_file_index]\n",
    "    forcing_data.append(read_pfb(file_to_read, keys=forcing_keys))\n",
    "\n",
    "# Now just concatenate together.\n",
    "# this works because we still have\n",
    "# the leftover time index at dimension 1\n",
    "# End result is dims of (n_forcing_vars, y, x)\n",
    "forcing_data = np.concatenate(forcing_data)\n",
    "forcing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all the parameter data and stacking it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 128, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_data = []\n",
    "for p, files in parameter_files.items():\n",
    "    parameter_data.append(read_pfb(files, keys=pressure_keys))\n",
    "\n",
    "# Concatenate the parameter data together\n",
    "# End result is a dims of (n_parameters, y, x)\n",
    "parameter_data = np.concatenate(parameter_data, axis=0)\n",
    "parameter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
