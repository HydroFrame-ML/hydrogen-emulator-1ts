{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "# Need to use xbatcher from: https://github.com/arbennett/xbatcher/tree/develop\n",
    " # See readme for installation instrutions \n",
    "import xbatcher as xb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scalers \n",
    "\n",
    "from glob import glob\n",
    "from parflow.tools.io import read_pfb\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParFlowDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, data_dir, run_name,\n",
    "        parameter_list, patch_size, overlap, scaler_yaml,\n",
    "        param_nlayer=np.zeros(len(parameter_list)), n_evaptrans=0\n",
    "        \n",
    "    ):\n",
    "        super().__init__() \n",
    "        self.base_dir = f'{data_dir}/{run_name}'\n",
    "        self.parameter_list = parameter_list\n",
    "        self.param_nlayer = param_nlayer #number of layers to use for each param, 0= use all, -n = n top layers, +n = n bottom layers\n",
    "        self.patch_size = patch_size\n",
    "        self.n_evaptrans = n_evaptrans\n",
    "        self.overlap = overlap\n",
    "        self.scaler_yaml = scaler_yaml\n",
    "        self.scaler = scalers.create_scalers_from_yaml(scaler_yaml)\n",
    "\n",
    "        self.pressure_files = sorted(glob(f'{self.base_dir}/transient/pressure*.pfb')) \n",
    "        self.pressure_files = {\n",
    "            't': self.pressure_files[0:-1],\n",
    "            't+1': self.pressure_files[1:]\n",
    "        }\n",
    "    \n",
    "        self.size_test = read_pfb(self.pressure_files['t'][0])\n",
    "        self.X_EXTENT = self.size_test.shape[2] \n",
    "        self.Y_EXTENT = self.size_test.shape[1]\n",
    "        self.Z_EXTENT = self.size_test.shape[0]\n",
    "        self.T_EXTENT = 1 #Change this to the number of timesteps -- should this be input up top?\n",
    "      \n",
    "        # Create a dummy dataset that will be used to pull indices for reading subsets of the data\n",
    "        self.dummy_data = xr.Dataset().assign_coords({\n",
    "            'time': np.arange(self.T_EXTENT),\n",
    "            'z': np.arange(self.Z_EXTENT),\n",
    "            'y': np.arange(self.Y_EXTENT),\n",
    "            'x': np.arange(self.X_EXTENT)\n",
    "        })\n",
    "   \n",
    "        self.bgen = xb.BatchGenerator(\n",
    "            self.dummy_data,\n",
    "            input_dims={'x': self.patch_size, 'y': self.patch_size, 'time': 1},\n",
    "            input_overlap={'x': self.overlap, 'y': self.overlap},\n",
    "            return_partial=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bgen) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_indices = self.bgen[idx]\n",
    "\n",
    "        # Pulling the indices we need\n",
    "        time_index = sample_indices['time'].values[0]\n",
    "        x_min, x_max = sample_indices['x'].values[[0, -1]]\n",
    "        y_min, y_max = sample_indices['y'].values[[0, -1]]\n",
    "\n",
    "        # Setting up the keys dictionary\n",
    "        patch_keys = {\n",
    "            'x': {'start': x_min, 'stop': x_max+1},\n",
    "            'y': {'start': y_min, 'stop': y_max+1},\n",
    "        }\n",
    "    \n",
    "        # Construct the state data and scale it:\n",
    "        file_to_read = self.pressure_files['t'][time_index]\n",
    "        state_data = read_pfb(file_to_read, keys=patch_keys)\n",
    "        for k in range(self.Z_EXTENT):\n",
    "            state_data[k]= self.scaler[f'press_diff_{k}'].transform(state_data[k])\n",
    "\n",
    "        # Construct the target data and scale it:\n",
    "        file_to_read_target = self.pressure_files['t+1'][time_index]\n",
    "        target_data = read_pfb(file_to_read_target, keys=patch_keys)\n",
    "        for k in range(self.Z_EXTENT):\n",
    "            target_data[k]= self.scaler[f'press_diff_{k}'].transform(target_data[k])\n",
    "\n",
    "        # Construct the parameter data and scale it:\n",
    "        parameter_data = []\n",
    "        for (parameter, n_lay) in zip(self.parameter_list, self.param_nlayer):\n",
    "            file_name=f'{self.base_dir}/static/{parameter}.pfb'\n",
    "            param_temp = read_pfb(file_name, keys=patch_keys)\n",
    "\n",
    "            #Scale the data\n",
    "            if param_temp.shape[0] == 1:\n",
    "                param_temp = self.scaler[f'{parameter}'].transform(param_temp)\n",
    "            else: \n",
    "                for k in range(param_temp.shape[0]):\n",
    "                    param_temp[k]= self.scaler[f'{parameter}_{k}'].transform(param_temp[k])\n",
    "\n",
    "                #Grab the top n bottom or top layers if specified in the param_nlayer list\n",
    "                #Grab the bottom n_lay layers\n",
    "                if n_lay > 0:\n",
    "                    param_temp = param_temp[0:n_lay,:,:]\n",
    "                #Grab the top n_lay layers\n",
    "                elif n_lay < 0:\n",
    "                    param_temp = param_temp[n_lay:,:,:]\n",
    "\n",
    "            parameter_data.append(param_temp)\n",
    "           \n",
    "\n",
    "        # Concatenate the parameter data together\n",
    "        # End result is a dims of (n_parameters, y, x)\n",
    "        parameter_data = np.concatenate(parameter_data, axis=0)\n",
    "\n",
    "        #Construct the evaptrans data and scale it\n",
    "        file_name_et=file_to_read.replace('pressure', 'evaptrans')\n",
    "        evaptrans = (read_pfb(file_name_et, keys= patch_keys))\n",
    "        for k in range(self.Z_EXTENT):\n",
    "            evaptrans[k]= self.scaler[f'evaptrans_{k}'].transform(evaptrans[k])\n",
    "        #Grab the top n bottom or top layers if specified in the param_nlayer list\n",
    "        #Grab the bottom n_lay layers\n",
    "        if self.n_evaptrans > 0:\n",
    "            evaptrans = evaptrans[0:self.n_evaptrans,:,:]\n",
    "        #Grab the top n_lay layers\n",
    "        elif self.n_evaptrans < 0:\n",
    "            evaptrans = evaptrans[self.n_evaptrans:,:,:]\n",
    "        \n",
    "        # Concatenate the state data with the parameter data\n",
    "        # End result is a dims of (sum(n_parameters*param_nlayer) + n_evaptrans + nz, y, x) \n",
    "        state_data = np.concatenate([state_data, evaptrans, parameter_data], axis=0)\n",
    "\n",
    "\n",
    "        return state_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 64 64\n",
      "(25, 4, 4) (10, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "parameter_list= ['slope_x', 'slope_y', 'permeability_x', 'permeability_y' ,     'permeability_z',\n",
    "'porosity',\n",
    "'pf_flowbarrier', 'mannings', 'specific_storage', 'sres' , 'ssat']\n",
    "\n",
    "ds = ParFlowDataset(\n",
    "    data_dir='/home/lc2465/NAIRR/',\n",
    "    run_name='test_box2_conus2_2002WY',\n",
    "    parameter_list=parameter_list, \n",
    "    n_evaptrans=-4,\n",
    "    param_nlayer = [0,0,1,1,1,1,1,0,1,1,1],\n",
    "    patch_size=16,\n",
    "    overlap=4,\n",
    "    scaler_yaml = 'default_scalers.yaml'\n",
    ")\n",
    "\n",
    "print(ds.size_test.shape[0], ds.size_test.shape[1], ds.size_test.shape[2])\n",
    "# Try to grab a sample - \n",
    "# How do we know how many samples we have generated? \n",
    "x, y = ds[5]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
