{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_cuda' from 'torch._utils' (/Users/laura/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mparflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_pfb\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[0;32m~/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/__init__.py:1706\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;66;03m# needs to be after torch.Tensor is defined to avoid circular dependencies\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m storage \u001b[38;5;28;01mas\u001b[39;00m storage  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1708\u001b[0m     _LegacyStorage,\n\u001b[1;32m   1709\u001b[0m     _StorageBase,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     UntypedStorage,\n\u001b[1;32m   1713\u001b[0m )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;66;03m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;66;03m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/storage.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _type, _cuda, _hpu\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Storage\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast, Any, Dict \u001b[38;5;28;01mas\u001b[39;00m _Dict, Optional \u001b[38;5;28;01mas\u001b[39;00m _Optional, TypeVar, Type, Union\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_cuda' from 'torch._utils' (/Users/laura/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/_utils.py)"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "# Need to use xbatcher from: https://github.com/arbennett/xbatcher/tree/develop\n",
    " # See readme for installation instrutions \n",
    "import xbatcher as xb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scalers \n",
    "\n",
    "from glob import glob\n",
    "from parflow.tools.io import read_pfb\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_cuda' from 'torch._utils' (/Users/laura/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/__init__.py:1706\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;66;03m# needs to be after torch.Tensor is defined to avoid circular dependencies\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m storage \u001b[38;5;28;01mas\u001b[39;00m storage  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1708\u001b[0m     _LegacyStorage,\n\u001b[1;32m   1709\u001b[0m     _StorageBase,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     UntypedStorage,\n\u001b[1;32m   1713\u001b[0m )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;66;03m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;66;03m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/storage.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _type, _cuda, _hpu\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Storage\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast, Any, Dict \u001b[38;5;28;01mas\u001b[39;00m _Dict, Optional \u001b[38;5;28;01mas\u001b[39;00m _Optional, TypeVar, Type, Union\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_cuda' from 'torch._utils' (/Users/laura/miniconda3/envs/hydrogen_emulator/lib/python3.10/site-packages/torch/_utils.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.__file__\n",
    "#Always install torch as soon as possible in the environment build process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParFlowDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, data_dir, run_name,\n",
    "        parameter_list, param_nlayer, n_evaptrans,\n",
    "        patch_size, overlap, scaler_yaml\n",
    "    ):\n",
    "        super().__init__() \n",
    "        self.base_dir = f'{data_dir}/{run_name}'\n",
    "        self.parameter_list = parameter_list\n",
    "        self.param_nlayer = param_nlayer #note could have this default to all 0 \n",
    "        self.patch_size = patch_size\n",
    "        self.n_evaptrans = n_evaptrans\n",
    "        self.overlap = overlap\n",
    "        self.scaler_yaml = scaler_yaml\n",
    "        self.scaler = scalers.create_scalers_from_yaml(scaler_yaml)\n",
    "\n",
    "        self.pressure_files = sorted(glob.glob(f'{self.base_dir}/transient/pressure*.pfb')) \n",
    "        self.pressure_files = {\n",
    "            't': self.pressure_files[0:-1],\n",
    "            't+1': self.pressure_files[1:]\n",
    "        }\n",
    "    \n",
    "        self.size_test = read_pfb(self.pressure_files['t'][0])\n",
    "        X_EXTENT = self.size_test.shape[2] \n",
    "        Y_EXTENT = self.size_test.shape[1]\n",
    "        Z_EXTENT = self.size_test.shape[0]\n",
    "        T_EXTENT = 1 #Change this to the number of timesteps -- should this be input up top?\n",
    "      \n",
    "        # Create a dummy dataset that will be used to pull indices for reading subsets of the data\n",
    "        self.dummy_data = xr.Dataset().assign_coords({\n",
    "            'time': np.arange(T_EXTENT),\n",
    "            'z': np.arange(Z_EXTENT),\n",
    "            'y': np.arange(Y_EXTENT),\n",
    "            'x': np.arange(X_EXTENT)\n",
    "        })\n",
    "   \n",
    "        self.bgen = xb.BatchGenerator(\n",
    "            self.dummy_data,\n",
    "            input_dims={'x': self.patch_size, 'y': self.patch_size, 'time': 1},\n",
    "            input_overlap={'x': self.overlap, 'y': self.overlap},\n",
    "            return_partial=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bgen) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_indices = self.bgen[idx]\n",
    "\n",
    "        # Pulling the indices we need\n",
    "        time_index = sample_indices['time'].values[0]\n",
    "        x_min, x_max = sample_indices['x'].values[[0, -1]]\n",
    "        y_min, y_max = sample_indices['y'].values[[0, -1]]\n",
    "\n",
    "        # Setting up the keys dictionary\n",
    "        patch_keys = {\n",
    "            'x': {'start': x_min, 'stop': x_max+1},\n",
    "            'y': {'start': y_min, 'stop': y_max+1},\n",
    "        }\n",
    "    \n",
    "        # Construct the state data and scale it:\n",
    "        file_to_read = self.pressure_files['t'][time_index]\n",
    "        state_data = read_pfb(file_to_read, keys=patch_keys)\n",
    "        for k in range(Z_EXTENT):\n",
    "            state_data[k]= scalers[f'press_diff_{k}'].transform(state_data[k])\n",
    "\n",
    "        # Construct the target data and scale it:\n",
    "        file_to_read_target = self.pressure_files['t+1'][time_index]\n",
    "        target_data = read_pfb(file_to_read_target, keys=patch_keys)\n",
    "        for k in range(Z_EXTENT):\n",
    "            target_data[k]= scalers[f'press_diff_{k}'].transform(target_data[k])\n",
    "\n",
    "        # Construct the parameter data and scale it:\n",
    "        parameter_data = []\n",
    "        for (parameter, n_lay) in zip(self.parameter_list, self.param_nlayer):\n",
    "            file_name=f'{self.base_dir}/static/{parameter}.pfb'\n",
    "            param_temp = read_pfb(file_name, keys=patch_keys)\n",
    "\n",
    "            #Scale the data\n",
    "            if param_temp.shape[0] == 1:\n",
    "                param_temp = scalers[f'{parameter}'].transform(param_temp)\n",
    "            else: \n",
    "                for k in range(param_temp.shape[0]):\n",
    "                    param_temp[k]= scalers[f'{parameter}_{k}'].transform(param_temp[k])\n",
    "\n",
    "                #Grab the top n bottom or top layers if specified in the param_nlayer list\n",
    "                #Grab the bottom n_lay layers\n",
    "                if n_lay > 0:\n",
    "                    param_temp = param_temp[0:n_lay,:,:]\n",
    "                #Grab the top n_lay layers\n",
    "                elif n_lay < 0:\n",
    "                    param_temp = param_temp[n_lay:,:,:]\n",
    "\n",
    "            parameter_data.append(param_temp)\n",
    "           \n",
    "\n",
    "        # Concatenate the parameter data together\n",
    "        # End result is a dims of (n_parameters, y, x)\n",
    "        parameter_data = np.concatenate(parameter_data, axis=0)\n",
    "\n",
    "        #Construct the evaptrans data and scale it\n",
    "        file_name_et=file_to_read.replace('pressure', 'evaptrans')\n",
    "        evaptrans = (read_pfb(file_name_et, keys= patch_keys))\n",
    "        for k in range(Z_EXTENT):\n",
    "            evaptrans[k]= scalers[f'evaptrans_{k}'].transform(evaptrans[k])\n",
    "        #Grab the top n bottom or top layers if specified in the param_nlayer list\n",
    "        #Grab the bottom n_lay layers\n",
    "        if self.n_evaptrans > 0:\n",
    "            evaptrans = evaptrans[0:self.n_evaptrans,:,:]\n",
    "        #Grab the top n_lay layers\n",
    "        elif self.n_evaptrans < 0:\n",
    "            evaptrans = evaptrans[self.n_evaptrans:,:,:]\n",
    "        \n",
    "        # Concatenate the state data with the parameter data\n",
    "        # End result is a dims of (n_parameters + 2*nz, y, x) \n",
    "        state_data = np.concatenate([state_data, evaptrans, parameter_data], axis=0)\n",
    "\n",
    "\n",
    "        return state_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "parameter_list= ['slope_x', 'slope_y', 'permeability_x', 'permeability_y' ,     'permeability_z',\n",
    "'porosity',\n",
    "'pf_flowbarrier', 'mannings', 'specific_storage', 'sres' , 'ssat']\n",
    "\n",
    "ds = ParFlowDataset(\n",
    "    data_dir='/Users/laura/Documents/Research/NAIRR/',\n",
    "    run_name='test_box2_conus2_2002WY',\n",
    "    parameter_list=parameter_list,\n",
    "    patch_size=16,\n",
    "    overlap=4\n",
    ")\n",
    "\n",
    "# Try to grab a sample - \n",
    "# How do we know how many samples we have generated? \n",
    "x, y = ds[5]\n",
    "print(x.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrogen_emulator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
